{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "LanguageList_Shao = [\n",
    "    'HEBREW',\n",
    "    'ARABIC',\n",
    "    'PORTUGUESE',\n",
    "    'ITALIAN',\n",
    "    'FRENCH',\n",
    "    'SPANISH',\n",
    "    'GERMAN',\n",
    "    'ENGLISH',\n",
    "    'RUSSIAN',\n",
    "    'FINNISH',\n",
    "    'VIETNAMESE',\n",
    "    'KOREAN',\n",
    "    'CHINESE',\n",
    "    'JAPANESE'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-10 20:55:16,566 Reading data from /Users/qier/.flair/datasets/ud_chinese\n",
      "2020-12-10 20:55:16,570 Train: /Users/qier/.flair/datasets/ud_chinese/zh_gsd-ud-train.conllu\n",
      "2020-12-10 20:55:16,574 Dev: /Users/qier/.flair/datasets/ud_chinese/zh_gsd-ud-dev.conllu\n",
      "2020-12-10 20:55:16,578 Test: /Users/qier/.flair/datasets/ud_chinese/zh_gsd-ud-test.conllu\n"
     ]
    }
   ],
   "source": [
    "import flair\n",
    "language = 'CHINESE'\n",
    "corpus = flair.datasets.UD_CHINESE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LanguageList_C1 = ['CHINESE','JAPANESE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-08 19:51:28,351 Reading data from /Users/qier/.flair/datasets/ud_chinese\n",
      "2020-12-08 19:51:28,356 Train: /Users/qier/.flair/datasets/ud_chinese/zh_gsd-ud-train.conllu\n",
      "2020-12-08 19:51:28,357 Dev: /Users/qier/.flair/datasets/ud_chinese/zh_gsd-ud-dev.conllu\n",
      "2020-12-08 19:51:28,358 Test: /Users/qier/.flair/datasets/ud_chinese/zh_gsd-ud-test.conllu\n",
      "2020-12-08 19:51:31,283 Reading data from /Users/qier/.flair/datasets/ud_chinese\n",
      "2020-12-08 19:51:31,284 Train: /Users/qier/.flair/datasets/ud_chinese/zh_gsd-ud-train.conllu\n",
      "2020-12-08 19:51:31,284 Dev: /Users/qier/.flair/datasets/ud_chinese/zh_gsd-ud-dev.conllu\n",
      "2020-12-08 19:51:31,285 Test: /Users/qier/.flair/datasets/ud_chinese/zh_gsd-ud-test.conllu\n"
     ]
    }
   ],
   "source": [
    "from flair import datasets\n",
    "language = 'CHINESE'\n",
    "# corpus_split   = eval('datasets.'+'UD_'+language + \"(split_multiwords=True)\")\n",
    "corpus_unsplit = eval('datasets.'+'UD_'+language + \"(split_multiwords=False)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_boundary_tag(corpus_element):\n",
    "    for token in corpus_element.tokens:\n",
    "        if len(token.text)==1: token.add_tag(tag_value='S',tag_type='boundary')\n",
    "        else: token.add_tag(tag_value='B'+'I'*(len(token.text)-2)+'E',tag_type='boundary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "corpus=corpus_unsplit.downsample(0.1)\n",
    "print(len(corpus.train.dataset))\n",
    "print(len(corpus.get_all_sentences().datasets[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.05902719497680664 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# prepare train and test data_sentence\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "for corpus_element in corpus.get_all_sentences().datasets[0]:\n",
    "        token_boundary_tag(corpus_element)\n",
    "        \n",
    "for corpus_element in corpus.get_all_sentences().datasets[1]:\n",
    "        token_boundary_tag(corpus_element)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentence = [corpus_element.to_plain_string() for corpus_element in corpus.get_all_sentences().datasets[0]]\n",
    "test_sentence = [corpus_element.to_plain_string() for corpus_element in corpus.get_all_sentences().datasets[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BE (1.0)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.get_all_sentences().datasets[0][0][1].get_tag('boundary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tag,test_tag =[],[]\n",
    "for element in corpus.get_all_sentences().datasets[0]:\n",
    "    temp_tag = [i.get_tag('boundary').value for i in element]\n",
    "    tag = ''\n",
    "    for i in temp_tag: tag += i\n",
    "    train_tag.append(tag)\n",
    "for element in corpus.get_all_sentences().datasets[1]:\n",
    "    temp_tag = [i.get_tag('boundary').value for i in element]\n",
    "    tag = ''\n",
    "    for i in temp_tag: tag += i\n",
    "    test_tag.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "該頻道目前主要是通過長城平台向海外地區傳播。\n",
      "SBEBEBESBEBEBESBEBEBES\n"
     ]
    }
   ],
   "source": [
    "print(train_sentence[0])\n",
    "print(train_tag[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get X Tag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-08 19:44:48,544 Reading data from /Users/qier/.flair/datasets/ud_german\n",
      "2020-12-08 19:44:48,548 Train: /Users/qier/.flair/datasets/ud_german/de_gsd-ud-train.conllu\n",
      "2020-12-08 19:44:48,549 Dev: /Users/qier/.flair/datasets/ud_german/de_gsd-ud-dev.conllu\n",
      "2020-12-08 19:44:48,550 Test: /Users/qier/.flair/datasets/ud_german/de_gsd-ud-test.conllu\n",
      "2020-12-08 19:45:13,562 Reading data from /Users/qier/.flair/datasets/ud_german\n",
      "2020-12-08 19:45:13,565 Train: /Users/qier/.flair/datasets/ud_german/de_gsd-ud-train.conllu\n",
      "2020-12-08 19:45:13,567 Dev: /Users/qier/.flair/datasets/ud_german/de_gsd-ud-dev.conllu\n",
      "2020-12-08 19:45:13,568 Test: /Users/qier/.flair/datasets/ud_german/de_gsd-ud-test.conllu\n"
     ]
    }
   ],
   "source": [
    "from flair import datasets\n",
    "language = 'GERMAN'\n",
    "corpus_split   = eval('datasets.'+'UD_'+language + \"(split_multiwords=True)\")\n",
    "corpus_unsplit = eval('datasets.'+'UD_'+language + \"(split_multiwords=False)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_unsplit.get_all_sentences()[0][0].whitespace_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.159074068069458 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# prepare train and test data_sentence\n",
    "corpus = corpus_unsplit.downsample(0.1)\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "for corpus_element in corpus.get_all_sentences().datasets[0]:\n",
    "        token_boundary_tag(corpus_element)\n",
    "        \n",
    "for corpus_element in corpus.get_all_sentences().datasets[1]:\n",
    "        token_boundary_tag(corpus_element)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentence = [corpus_element.to_plain_string() for corpus_element in corpus.get_all_sentences().datasets[0]]\n",
    "test_sentence = [corpus_element.to_plain_string() for corpus_element in corpus.get_all_sentences().datasets[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nette Gespräche, klasse Ergebnis'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentence[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flair.data.Token"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "element = corpus.get_all_sentences().datasets[0][0]\n",
    "token = element[0]\n",
    "token.whitespace_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tag,test_tag =[],[]\n",
    "for element in corpus.get_all_sentences().datasets[0]:\n",
    "    tag = ''\n",
    "    for token in element:\n",
    "        tag += token.get_tag('boundary').value\n",
    "        if token.whitespace_after == True:\n",
    "            tag +='X'\n",
    "    train_tag.append(tag[:-1]) # the last token of a sentence have Whitespace_after == True, though it should be False\n",
    "for element in corpus.get_all_sentences().datasets[1]:\n",
    "    tag = ''\n",
    "    for token in element:\n",
    "        tag += token.get_tag('boundary').value\n",
    "        if token.whitespace_after == True:\n",
    "            tag +='X'\n",
    "    test_tag.append(tag[:-1]) # the last token of a sentence have Whitespace_after == True, though it should be False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Was soll ich sagen... netter Empfang, Möbel nicht die neusten aber OK.\n",
      "BIEXBIIEXBIEXBIIIEBIEXBIIIIEXBIIIIIESXBIIIEXBIIIEXBIEXBIIIIIEXBIIEXBES\n"
     ]
    }
   ],
   "source": [
    "print(train_sentence[15])\n",
    "print(train_tag[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = zip(train_sentence,train_tag)\n",
    "train_data=list(train_data)\n",
    "\n",
    "test_data = zip(test_sentence,test_tag)\n",
    "test_data=list(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./data/%s_Train.pickle'%language, 'wb') as f:\n",
    "    pickle.dump(train_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/%s_Train.pickle'%language, 'rb') as f:\n",
    "     data_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to tag every language in the List and export training and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "LanguageList_Shao = [\n",
    "    'HEBREW',\n",
    "    'ARABIC',\n",
    "    'PORTUGUESE',\n",
    "    'ITALIAN',\n",
    "    'FRENCH',\n",
    "    'SPANISH',\n",
    "    'GERMAN',\n",
    "    'ENGLISH',\n",
    "    'RUSSIAN',\n",
    "    'FINNISH',\n",
    "    'VIETNAMESE',\n",
    "    'KOREAN',\n",
    "    'CHINESE',\n",
    "    'JAPANESE'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_boundary_tag_BIESX(corpus_element):\n",
    "    for token in corpus_element.tokens:\n",
    "        if len(token.text)==1: token.add_tag(tag_value='S',tag_type='BIESX')\n",
    "        else: token.add_tag(tag_value='B'+'I'*(len(token.text)-2)+'E',tag_type='BIESX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-08 21:03:54,928 Reading data from /Users/qier/.flair/datasets/ud_hebrew\n",
      "2020-12-08 21:03:54,931 Train: /Users/qier/.flair/datasets/ud_hebrew/he_htb-ud-train.conllu\n",
      "2020-12-08 21:03:54,936 Dev: /Users/qier/.flair/datasets/ud_hebrew/he_htb-ud-dev.conllu\n",
      "2020-12-08 21:03:54,940 Test: /Users/qier/.flair/datasets/ud_hebrew/he_htb-ud-test.conllu\n",
      "--- 55.3483350276947 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 1/14 [00:55<12:03, 55.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-08 21:04:50,549 Reading data from /Users/qier/.flair/datasets/ud_arabic\n",
      "2020-12-08 21:04:50,551 Train: /Users/qier/.flair/datasets/ud_arabic/ar_padt-ud-train.conllu\n",
      "2020-12-08 21:04:50,552 Dev: /Users/qier/.flair/datasets/ud_arabic/ar_padt-ud-dev.conllu\n",
      "2020-12-08 21:04:50,553 Test: /Users/qier/.flair/datasets/ud_arabic/ar_padt-ud-test.conllu\n",
      "--- 27.4900062084198 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█▍        | 2/14 [01:23<09:28, 47.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-08 21:05:18,759 Reading data from /Users/qier/.flair/datasets/ud_portuguese\n",
      "2020-12-08 21:05:18,761 Train: /Users/qier/.flair/datasets/ud_portuguese/pt_bosque-ud-train.conllu\n",
      "2020-12-08 21:05:18,772 Dev: /Users/qier/.flair/datasets/ud_portuguese/pt_bosque-ud-dev.conllu\n",
      "2020-12-08 21:05:18,774 Test: /Users/qier/.flair/datasets/ud_portuguese/pt_bosque-ud-test.conllu\n",
      "--- 19.999699115753174 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|██▏       | 3/14 [01:44<07:13, 39.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-08 21:05:39,391 Reading data from /Users/qier/.flair/datasets/ud_italian\n",
      "2020-12-08 21:05:39,398 Train: /Users/qier/.flair/datasets/ud_italian/it_isdt-ud-train.conllu\n",
      "2020-12-08 21:05:39,399 Dev: /Users/qier/.flair/datasets/ud_italian/it_isdt-ud-dev.conllu\n",
      "2020-12-08 21:05:39,400 Test: /Users/qier/.flair/datasets/ud_italian/it_isdt-ud-test.conllu\n",
      "--- 72.86845898628235 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|██▊       | 4/14 [02:58<08:17, 49.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-08 21:06:53,239 Reading data from /Users/qier/.flair/datasets/ud_french\n",
      "2020-12-08 21:06:53,244 Train: /Users/qier/.flair/datasets/ud_french/fr_gsd-ud-train.conllu\n",
      "2020-12-08 21:06:53,246 Dev: /Users/qier/.flair/datasets/ud_french/fr_gsd-ud-dev.conllu\n",
      "2020-12-08 21:06:53,248 Test: /Users/qier/.flair/datasets/ud_french/fr_gsd-ud-test.conllu\n",
      "--- 54.41636300086975 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███▌      | 5/14 [03:54<07:44, 51.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-08 21:07:49,148 Reading data from /Users/qier/.flair/datasets/ud_spanish\n",
      "2020-12-08 21:07:49,168 Train: /Users/qier/.flair/datasets/ud_spanish/es_gsd-ud-train.conllu\n",
      "2020-12-08 21:07:49,170 Dev: /Users/qier/.flair/datasets/ud_spanish/es_gsd-ud-dev.conllu\n",
      "2020-12-08 21:07:49,178 Test: /Users/qier/.flair/datasets/ud_spanish/es_gsd-ud-test.conllu\n",
      "--- 66.8886170387268 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|████▎     | 6/14 [05:02<07:32, 56.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-08 21:08:57,211 Reading data from /Users/qier/.flair/datasets/ud_german\n",
      "2020-12-08 21:08:57,216 Train: /Users/qier/.flair/datasets/ud_german/de_gsd-ud-train.conllu\n",
      "2020-12-08 21:08:57,220 Dev: /Users/qier/.flair/datasets/ud_german/de_gsd-ud-dev.conllu\n",
      "2020-12-08 21:08:57,222 Test: /Users/qier/.flair/datasets/ud_german/de_gsd-ud-test.conllu\n",
      "--- 93.0318353176117 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 7/14 [06:36<07:53, 67.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-08 21:10:31,046 Reading data from /Users/qier/.flair/datasets/ud_english\n",
      "2020-12-08 21:10:31,054 Train: /Users/qier/.flair/datasets/ud_english/en_ewt-ud-train.conllu\n",
      "2020-12-08 21:10:31,055 Dev: /Users/qier/.flair/datasets/ud_english/en_ewt-ud-dev.conllu\n",
      "2020-12-08 21:10:31,058 Test: /Users/qier/.flair/datasets/ud_english/en_ewt-ud-test.conllu\n",
      "--- 95.42424082756042 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|█████▋    | 8/14 [08:12<07:37, 76.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-08 21:12:07,195 Reading data from /Users/qier/.flair/datasets/ud_russian\n",
      "2020-12-08 21:12:07,206 Train: /Users/qier/.flair/datasets/ud_russian/ru_syntagrus-ud-train.conllu\n",
      "2020-12-08 21:12:07,209 Dev: /Users/qier/.flair/datasets/ud_russian/ru_syntagrus-ud-dev.conllu\n",
      "2020-12-08 21:12:07,220 Test: /Users/qier/.flair/datasets/ud_russian/ru_syntagrus-ud-test.conllu\n",
      "--- 387.2764928340912 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|██████▍   | 9/14 [14:43<14:14, 170.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-08 21:18:38,743 Reading data from /Users/qier/.flair/datasets/ud_finnish\n",
      "2020-12-08 21:18:38,762 Train: /Users/qier/.flair/datasets/ud_finnish/fi_tdt-ud-train.conllu\n",
      "2020-12-08 21:18:38,773 Dev: /Users/qier/.flair/datasets/ud_finnish/fi_tdt-ud-dev.conllu\n",
      "2020-12-08 21:18:38,774 Test: /Users/qier/.flair/datasets/ud_finnish/fi_tdt-ud-test.conllu\n",
      "--- 7.120421886444092 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|███████▏  | 10/14 [14:51<08:07, 121.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-08 21:18:46,430 Reading data from /Users/qier/.flair/datasets/ud_vietnamese\n",
      "2020-12-08 21:18:46,431 Train: /Users/qier/.flair/datasets/ud_vietnamese/vi_vtb-ud-train.conllu\n",
      "2020-12-08 21:18:46,432 Dev: /Users/qier/.flair/datasets/ud_vietnamese/vi_vtb-ud-dev.txt\n",
      "2020-12-08 21:18:46,433 Test: /Users/qier/.flair/datasets/ud_vietnamese/vi_vtb-ud-test.conllu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|███████▊  | 11/14 [14:52<04:17, 85.74s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1.2890081405639648 seconds ---\n",
      "2020-12-08 21:18:47,816 Reading data from /Users/qier/.flair/datasets/ud_korean\n",
      "2020-12-08 21:18:47,817 Train: /Users/qier/.flair/datasets/ud_korean/ko_kaist-ud-train.conllu\n",
      "2020-12-08 21:18:47,817 Dev: /Users/qier/.flair/datasets/ud_korean/ko_kaist-ud-dev.conllu\n",
      "2020-12-08 21:18:47,821 Test: /Users/qier/.flair/datasets/ud_korean/ko_kaist-ud-test.conllu\n",
      "--- 167.32113027572632 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████▌ | 12/14 [17:41<03:40, 110.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-08 21:21:36,043 Reading data from /Users/qier/.flair/datasets/ud_chinese\n",
      "2020-12-08 21:21:36,046 Train: /Users/qier/.flair/datasets/ud_chinese/zh_gsd-ud-train.conllu\n",
      "2020-12-08 21:21:36,047 Dev: /Users/qier/.flair/datasets/ud_chinese/zh_gsd-ud-dev.conllu\n",
      "2020-12-08 21:21:36,049 Test: /Users/qier/.flair/datasets/ud_chinese/zh_gsd-ud-test.conllu\n",
      "--- 3.8061439990997314 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|█████████▎| 13/14 [17:45<01:18, 78.56s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-08 21:21:40,102 Reading data from /Users/qier/.flair/datasets/ud_japanese\n",
      "2020-12-08 21:21:40,103 Train: /Users/qier/.flair/datasets/ud_japanese/ja_gsd-ud-train.conllu\n",
      "2020-12-08 21:21:40,105 Dev: /Users/qier/.flair/datasets/ud_japanese/ja_gsd-ud-dev.conllu\n",
      "2020-12-08 21:21:40,106 Test: /Users/qier/.flair/datasets/ud_japanese/ja_gsd-ud-test.conllu\n",
      "--- 27.2071430683136 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [18:12<00:00, 78.06s/it]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "for language in tqdm(LanguageList_Shao):\n",
    "    start_time = time.time()\n",
    "#     corpus_split   = eval('datasets.'+'UD_'+language + \"(split_multiwords=True)\")\n",
    "    corpus_unsplit = eval('datasets.'+'UD_'+language + \"(split_multiwords=False)\")\n",
    "    corpus = corpus_unsplit\n",
    "\n",
    "    # tag token \n",
    "    for corpus_element in corpus.get_all_sentences().datasets[0]:\n",
    "            token_boundary_tag(corpus_element)\n",
    "\n",
    "    for corpus_element in corpus.get_all_sentences().datasets[1]:\n",
    "            token_boundary_tag(corpus_element)\n",
    "\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "    # prepare train and test sentence list\n",
    "    train_sentence = [corpus_element.to_plain_string() for corpus_element in corpus.get_all_sentences().datasets[0]]\n",
    "    test_sentence = [corpus_element.to_plain_string() for corpus_element in corpus.get_all_sentences().datasets[1]]\n",
    "    \n",
    "    # prepare train and test tag list\n",
    "    train_tag,test_tag =[],[]\n",
    "    for corpus_element in corpus.get_all_sentences().datasets[0]:\n",
    "        tag = ''\n",
    "        for token in corpus_element:\n",
    "            tag += token.get_tag('boundary').value\n",
    "            if token.whitespace_after == True:\n",
    "                tag +='X'\n",
    "        train_tag.append(tag[:-1]) # the last token of a sentence have Whitespace_after == True, though it should be False\n",
    "    for corpus_element in corpus.get_all_sentences().datasets[1]:\n",
    "        tag = ''\n",
    "        for token in corpus_element:\n",
    "            tag += token.get_tag('boundary').value\n",
    "            if token.whitespace_after == True:\n",
    "                tag +='X'\n",
    "        test_tag.append(tag[:-1]) # the last token of a sentence have Whitespace_after == True, though it should be False\n",
    "\n",
    "    # prepare train and test data by zip and save file \n",
    "    train_data = list(zip(train_sentence,train_tag))\n",
    "    test_data  = list(zip(test_sentence ,test_tag))\n",
    "    \n",
    "    import pickle\n",
    "    with open('./data/%s_Train.pickle'%language, 'wb') as f1:\n",
    "        pickle.dump(train_data, f1)\n",
    "    with open('./data/%s_Test.pickle'%language, 'wb') as f2:\n",
    "        pickle.dump(test_data, f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'JAPANESE'\n",
    "with open('./data/%s_Train.pickle'%language, 'rb') as f1:\n",
    "     data_train = pickle.load(f1)\n",
    "with open('./data/%s_Test.pickle'%language, 'rb') as f2:\n",
    "     data_test = pickle.load(f2)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train[15][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train[15][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEBREW\n",
      "16\n",
      "16\n",
      "ARABIC\n",
      "207\n",
      "207\n",
      "PORTUGUESE\n",
      "123\n",
      "123\n",
      "ITALIAN\n",
      "34\n",
      "34\n",
      "FRENCH\n",
      "83\n",
      "83\n",
      "SPANISH\n",
      "64\n",
      "64\n",
      "GERMAN\n",
      "235\n",
      "235\n",
      "ENGLISH\n",
      "126\n",
      "126\n",
      "RUSSIAN\n",
      "53\n",
      "53\n",
      "FINNISH\n",
      "7\n",
      "7\n",
      "VIETNAMESE\n",
      "121\n",
      "121\n",
      "KOREAN\n",
      "20\n",
      "20\n",
      "CHINESE\n",
      "119\n",
      "118\n",
      "JAPANESE\n",
      "26\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "for language in LanguageList_Shao:\n",
    "    with open('./data/%s_Train.pickle'%language, 'rb') as f1:\n",
    "         data_train = pickle.load(f1)\n",
    "    with open('./data/%s_Test.pickle'%language, 'rb') as f2:\n",
    "         data_test = pickle.load(f2)   \n",
    "    print(language)\n",
    "    print(len(data_train[15][0]))\n",
    "    print(len(data_train[15][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('מתחיל מסע הנקמה.', 'BIIIEXBIEXBIIIES')"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Correct for Chinese and Japanese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "LanguageList_C1 = ['CHINESE','JAPANESE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-09 13:20:24,653 Reading data from /Users/qier/.flair/datasets/ud_chinese\n",
      "2020-12-09 13:20:24,656 Train: /Users/qier/.flair/datasets/ud_chinese/zh_gsd-ud-train.conllu\n",
      "2020-12-09 13:20:24,657 Dev: /Users/qier/.flair/datasets/ud_chinese/zh_gsd-ud-dev.conllu\n",
      "2020-12-09 13:20:24,659 Test: /Users/qier/.flair/datasets/ud_chinese/zh_gsd-ud-test.conllu\n",
      "--- 12.84265685081482 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 1/2 [00:13<00:13, 13.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-09 13:20:37,760 Reading data from /Users/qier/.flair/datasets/ud_japanese\n",
      "2020-12-09 13:20:37,760 Train: /Users/qier/.flair/datasets/ud_japanese/ja_gsd-ud-train.conllu\n",
      "2020-12-09 13:20:37,761 Dev: /Users/qier/.flair/datasets/ud_japanese/ja_gsd-ud-dev.conllu\n",
      "2020-12-09 13:20:37,762 Test: /Users/qier/.flair/datasets/ud_japanese/ja_gsd-ud-test.conllu\n",
      "--- 4.5082879066467285 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:18<00:00,  9.02s/it]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "for language in tqdm(LanguageList_C1):\n",
    "    start_time = time.time()\n",
    "#     corpus_split   = eval('datasets.'+'UD_'+language + \"(split_multiwords=True)\")\n",
    "    corpus_unsplit = eval('datasets.'+'UD_'+language + \"(split_multiwords=False)\")\n",
    "    corpus = corpus_unsplit\n",
    "\n",
    "    # tag token \n",
    "    for corpus_element in corpus.get_all_sentences().datasets[0]:\n",
    "            token_boundary_tag(corpus_element)\n",
    "\n",
    "    for corpus_element in corpus.get_all_sentences().datasets[1]:\n",
    "            token_boundary_tag(corpus_element)\n",
    "\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "    # prepare train and test sentence list\n",
    "    train_sentence = [corpus_element.to_plain_string() for corpus_element in corpus.get_all_sentences().datasets[0]]\n",
    "    test_sentence = [corpus_element.to_plain_string() for corpus_element in corpus.get_all_sentences().datasets[1]]\n",
    "    \n",
    "    # prepare train and test tag list\n",
    "    train_tag,test_tag =[],[]\n",
    "    for corpus_element in corpus.get_all_sentences().datasets[0]:\n",
    "        tag = ''\n",
    "        for token in corpus_element:\n",
    "            tag += token.get_tag('boundary').value\n",
    "            if token.whitespace_after == True:\n",
    "                tag +='X'\n",
    "        train_tag.append(tag) # the last token of a sentence have Whitespace_after == False only correct for CN and JP\n",
    "    for corpus_element in corpus.get_all_sentences().datasets[1]:\n",
    "        tag = ''\n",
    "        for token in corpus_element:\n",
    "            tag += token.get_tag('boundary').value\n",
    "            if token.whitespace_after == True:\n",
    "                tag +='X'\n",
    "        test_tag.append(tag) # the last token of a sentence have Whitespace_after == False only correct for CN and JP\n",
    "\n",
    "    # prepare train and test data by zip and save file \n",
    "    train_data = list(zip(train_sentence,train_tag))\n",
    "    test_data  = list(zip(test_sentence ,test_tag))\n",
    "    \n",
    "    import pickle\n",
    "    with open('./data/%s_Train.pickle'%language, 'wb') as f1:\n",
    "        pickle.dump(train_data, f1)\n",
    "    with open('./data/%s_Test.pickle'%language, 'wb') as f2:\n",
    "        pickle.dump(test_data, f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHINESE\n",
      "47\n",
      "47\n",
      "JAPANESE\n",
      "35\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "for language in LanguageList_C1:\n",
    "    with open('./data/%s_Train.pickle'%language, 'rb') as f1:\n",
    "         data_train = pickle.load(f1)\n",
    "    with open('./data/%s_Test.pickle'%language, 'rb') as f2:\n",
    "         data_test = pickle.load(f2)   \n",
    "    print(language)\n",
    "    print(len(data_train[20][0]))\n",
    "    print(len(data_train[20][1]))"
   ]
  },
  {
   "source": [
    "### Correct for 'PORTUGUESE'"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-01-04 22:07:37,235 Reading data from /Users/qier/.flair/datasets/ud_vietnamese\n",
      "2021-01-04 22:07:37,236 Train: /Users/qier/.flair/datasets/ud_vietnamese/vi_vtb-ud-train.conllu\n",
      "2021-01-04 22:07:37,253 Dev: /Users/qier/.flair/datasets/ud_vietnamese/vi_vtb-ud-dev.txt\n",
      "2021-01-04 22:07:37,254 Test: /Users/qier/.flair/datasets/ud_vietnamese/vi_vtb-ud-test.conllu\n",
      "--- 3.31368088722229 seconds ---\n"
     ]
    }
   ],
   "source": [
    "language = 'PORTUGUESE'\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "start_time = time.time()\n",
    "corpus_unsplit = eval('datasets.'+'UD_'+language + \"(split_multiwords=False)\")\n",
    "corpus = corpus_unsplit\n",
    "\n",
    "# tag token \n",
    "for corpus_element in corpus.get_all_sentences().datasets[0]:\n",
    "        token_boundary_tag(corpus_element)\n",
    "\n",
    "for corpus_element in corpus.get_all_sentences().datasets[1]:\n",
    "        token_boundary_tag(corpus_element)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# prepare train and test sentence list\n",
    "train_sentence = [corpus_element.to_plain_string() for corpus_element in corpus.get_all_sentences().datasets[0]]\n",
    "test_sentence = [corpus_element.to_plain_string() for corpus_element in corpus.get_all_sentences().datasets[1]]\n",
    "\n",
    "# prepare train and test tag list\n",
    "train_tag,test_tag =[],[]\n",
    "for corpus_element in corpus.get_all_sentences().datasets[0]:\n",
    "    tag = ''\n",
    "    for token in corpus_element:\n",
    "        tag += token.get_tag('boundary').value\n",
    "        if token.whitespace_after == True:\n",
    "            tag +='X'\n",
    "    train_tag.append(tag[:-1]) # the last token of a sentence have Whitespace_after == False only correct for CN and JP\n",
    "for corpus_element in corpus.get_all_sentences().datasets[1]:\n",
    "    tag = ''\n",
    "    for token in corpus_element:\n",
    "        tag += token.get_tag('boundary').value\n",
    "        if token.whitespace_after == True:\n",
    "            tag +='X'\n",
    "    test_tag.append(tag[:-1]) # the last token of a sentence have Whitespace_after == False only correct for CN and JP\n",
    "\n",
    "# prepare train and test data by zip and save file \n",
    "train_data = list(zip(train_sentence,train_tag))\n",
    "test_data  = list(zip(test_sentence ,test_tag))\n",
    "\n",
    "# import pickle\n",
    "# with open('./data/%s_Train.pickle'%language, 'wb') as f1:\n",
    "#     pickle.dump(train_data, f1)\n",
    "# with open('./data/%s_Test.pickle'%language, 'wb') as f2:\n",
    "#     pickle.dump(test_data, f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_data)):\n",
    "    if len(train_data[i][0]) != len(train_data[i][1]):\n",
    "        print(i) "
   ]
  },
  {
   "source": [
    "for i in range(len(test_data)):\n",
    "    if len(test_data[i][0]) != len(test_data[i][1]):\n",
    "        print(i) "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PORTUGUESE 405\n",
      "PORTUGUESE 602\n",
      "PORTUGUESE 949\n",
      "PORTUGUESE 959\n",
      "PORTUGUESE 964\n",
      "PORTUGUESE 965\n",
      "PORTUGUESE 1745\n",
      "PORTUGUESE 2326\n",
      "PORTUGUESE 2789\n",
      "PORTUGUESE 2800\n",
      "PORTUGUESE 2889\n",
      "PORTUGUESE 2890\n",
      "PORTUGUESE 2934\n",
      "PORTUGUESE 3069\n",
      "PORTUGUESE 3241\n",
      "PORTUGUESE 3507\n",
      "PORTUGUESE 3545\n",
      "PORTUGUESE 3575\n",
      "PORTUGUESE 3872\n",
      "PORTUGUESE 4348\n",
      "PORTUGUESE 4426\n",
      "PORTUGUESE 4492\n",
      "PORTUGUESE 4805\n",
      "PORTUGUESE 4873\n",
      "PORTUGUESE 4988\n",
      "PORTUGUESE 5027\n",
      "PORTUGUESE 5139\n",
      "PORTUGUESE 5372\n",
      "PORTUGUESE 5382\n",
      "PORTUGUESE 6074\n",
      "PORTUGUESE 6219\n",
      "PORTUGUESE 6220\n",
      "PORTUGUESE 6833\n",
      "PORTUGUESE 6932\n",
      "PORTUGUESE 6939\n",
      "PORTUGUESE 7064\n",
      "PORTUGUESE 7085\n",
      "PORTUGUESE 7438\n",
      "PORTUGUESE 7653\n",
      "PORTUGUESE 7671\n",
      "PORTUGUESE 7672\n",
      "PORTUGUESE 7797\n",
      "PORTUGUESE 7837\n",
      "PORTUGUESE 7838\n",
      "PORTUGUESE 7840\n",
      "PORTUGUESE 20\n",
      "PORTUGUESE 35\n",
      "PORTUGUESE 237\n",
      "PORTUGUESE 391\n",
      "ITALIAN 1057\n",
      "ENGLISH 494\n",
      "ENGLISH 2489\n",
      "ENGLISH 2631\n",
      "ENGLISH 3124\n",
      "ENGLISH 3134\n",
      "ENGLISH 3144\n",
      "ENGLISH 5101\n",
      "ENGLISH 5297\n",
      "ENGLISH 5302\n",
      "ENGLISH 5993\n",
      "ENGLISH 6586\n",
      "ENGLISH 6734\n",
      "ENGLISH 6771\n",
      "ENGLISH 6792\n",
      "ENGLISH 7212\n",
      "ENGLISH 7247\n",
      "ENGLISH 7258\n",
      "ENGLISH 7259\n",
      "ENGLISH 7260\n",
      "ENGLISH 7261\n",
      "ENGLISH 7272\n",
      "ENGLISH 7273\n",
      "ENGLISH 7275\n",
      "ENGLISH 7385\n",
      "ENGLISH 7503\n",
      "ENGLISH 7941\n",
      "ENGLISH 7947\n",
      "ENGLISH 8219\n",
      "ENGLISH 8264\n",
      "ENGLISH 8314\n",
      "ENGLISH 8335\n",
      "ENGLISH 8373\n",
      "ENGLISH 8403\n",
      "ENGLISH 8437\n",
      "ENGLISH 8466\n",
      "ENGLISH 8510\n",
      "ENGLISH 8560\n",
      "ENGLISH 8569\n",
      "ENGLISH 8678\n",
      "ENGLISH 8832\n",
      "ENGLISH 8981\n",
      "ENGLISH 8983\n",
      "ENGLISH 9086\n",
      "ENGLISH 9255\n",
      "ENGLISH 9329\n",
      "ENGLISH 9393\n",
      "ENGLISH 9446\n",
      "ENGLISH 9522\n",
      "ENGLISH 9553\n",
      "ENGLISH 10003\n",
      "ENGLISH 10043\n",
      "ENGLISH 10115\n",
      "ENGLISH 10203\n",
      "ENGLISH 10205\n",
      "ENGLISH 10212\n",
      "ENGLISH 10265\n",
      "ENGLISH 10402\n",
      "ENGLISH 10745\n",
      "ENGLISH 11026\n",
      "ENGLISH 11048\n",
      "ENGLISH 11088\n",
      "ENGLISH 11216\n",
      "ENGLISH 11217\n",
      "ENGLISH 11218\n",
      "ENGLISH 11219\n",
      "ENGLISH 11524\n",
      "ENGLISH 11655\n",
      "ENGLISH 11656\n",
      "ENGLISH 11657\n",
      "ENGLISH 11658\n",
      "ENGLISH 11659\n",
      "ENGLISH 11660\n",
      "ENGLISH 11661\n",
      "ENGLISH 11662\n",
      "ENGLISH 11663\n",
      "ENGLISH 11664\n",
      "ENGLISH 11692\n",
      "ENGLISH 11719\n",
      "ENGLISH 11731\n",
      "ENGLISH 11831\n",
      "ENGLISH 11853\n",
      "ENGLISH 11915\n",
      "ENGLISH 12415\n",
      "ENGLISH 373\n",
      "ENGLISH 456\n",
      "ENGLISH 845\n",
      "ENGLISH 881\n",
      "ENGLISH 883\n",
      "ENGLISH 1030\n",
      "ENGLISH 1048\n",
      "ENGLISH 1087\n",
      "ENGLISH 1137\n",
      "ENGLISH 1214\n",
      "ENGLISH 1244\n",
      "ENGLISH 1294\n",
      "ENGLISH 1318\n",
      "ENGLISH 1502\n",
      "ENGLISH 1734\n",
      "ENGLISH 1771\n",
      "ENGLISH 1917\n",
      "ENGLISH 1984\n",
      "RUSSIAN 10118\n",
      "RUSSIAN 10994\n",
      "RUSSIAN 17115\n",
      "RUSSIAN 20716\n",
      "RUSSIAN 34731\n",
      "RUSSIAN 35244\n",
      "RUSSIAN 36634\n",
      "RUSSIAN 36729\n",
      "RUSSIAN 39042\n",
      "RUSSIAN 46420\n",
      "RUSSIAN 190\n",
      "RUSSIAN 3749\n",
      "139 24\n"
     ]
    }
   ],
   "source": [
    "### Check data Length"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}